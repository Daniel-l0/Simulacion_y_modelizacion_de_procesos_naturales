[MÚSICA] Así que hola. Continuamos con nuestra clase sobre modelado de simulación de procesos naturales. Y ahora me gustaría discutir nuestro segundo módulo sobre el método de Monte-Carlo y esto es lo que algunas personas llaman el Markov-Chain Monte-Carlo, MCMC. Así que el objetivo principal de nuevo del método de Monte-Carlo es muestrear procesos. Y, aquí sólo vamos a estudiar un proceso, un proceso estocástico, que en realidad está muestreando el espacio de estados. Y primero queremos entender la propiedad de este proceso y ver si podemos afinarlo. Sólo necesitaba muestrear realmente algo que usted quiere. Bien, entonces consideramos un proceso estocástico que se moverá a través del espacio de estados que no estoy especificando con precisión aquí pero le daré un ejemplo. ¿Y qué quiero decir con explorar? Significa que si tengo alguna posición x en este espacio de estados saltaré a la posición x prime en el siguiente paso temporal y la probabilidad de elegir x prime para saltar viene dada por la función de transición que aquí he llamado W de x a x prime. Y cada vez que he hecho un intento de saltar a algún sitio, he avanzado el tiempo del sistema una unidad. Así que esa es la forma en que evoluciona el tiempo en esto, y para aquellos de ustedes que recuerden su clase de matemáticas, esto se llama una cadena de Markov. Así que dando un pequeño ejemplo, puede que tenga aquí un sistema físico de partícula, por ejemplo, están en algún lugar en el espacio. Y básicamente, puede que quiera muestrear el espacio de la configuración de esta partícula de acuerdo con la física a la que obedecen y las ideas de esta configuración negra podría hacer modificación así que puedo decidir mover por ejemplo dos partícula al punto blanco, ¿de acuerdo? Y entonces eso es típicamente un salto en mi espacio de estados. Y la pregunta es con qué probabilidad debo aceptar este movimiento, de acuerdo. Y en realidad, tengo, por supuesto, una restricción para aceptar o rechazar este cambio, es que me gustaría aceptar cambios que correspondan a un movimiento natural de esta partícula. Así que sabe que el gas está hecho, por supuesto, de partículas. Pero eso no es ciertamente drástico como sugiere mi imagen. Siguen moviendose, pero no exploran toda la configuracion, porque tipicamente habia una temperatura dada. Y aqui tenemos un ejemplo donde suponemos por ejemplo que esta molécula, estan en el sistema descrito por la distribucion de Maxwell-Boltzmann , que es la energia de la configuracion x, que depende de toda la posicion de la particula y de la temperatura a la que se mantiene el sistema. Entonces la pregunta es, ¿hay alguna forma de elegir esta probabilidad de transición? De forma que pueda mover mi sistema de una configuración a otra siguiendo respetando la distribución de probabilidad impuesta por la física, por ejemplo u otra que yo conozca. Bien, pues hagamos un poco de matemáticas y la probabilidad de que nuestro proceso de exploración tenga la posición x en el espacio de estado en el tiempo t más 1 es básicamente la probabilidad de que estuviera en otro lugar, algún primo x, en el tiempo t, y entonces se salta de primo x a x, así que eso es teoría de la probabilidad muy simple para procesos de Markov. Así que ahora les daré un ejemplo sencillo de ello. Consideraremos un espacio D que no es más que una discretización de la línea horizontal, lo que significa que puedo tener una partícula que puede saltar de una celda a la siguiente, a la izquierda, a la derecha, o permanecer en reposo. O digamos que el espacio discreto es el símbolo, Z, número natural, y entonces quiero muestrear este estado. Así que asumo que sólo pueden moverse a la derecha, que probabilidad llamo W+ para ir a la dirección positiva. O puedo mover a la izquierda, o posición negativa como probabilidad W-, o puedo permanecer en reposo con la propiedad W0 y por supuesto la suma de esta cosa debe ser 1. Así que ahora, mi ecuación aquí se convierte con este conjunto reducido de salto a simplemente esta que sólo tres posibilidad de evolucionar, así que si estoy posición x, yo tiempo t + 1, yo era posición anterior x- 1, y luego salto a la derecha, o yo estaba en la posición x y no me moví, o yo estaba en la posición x + 1 y me moví a la izquierda, ¿de acuerdo? Así que, ahora supongamos que quiero usar este proceso estocástico para muestrear la ecuación de difusión. Así que usted aprenderá más acerca de la ecuación de difusión en las próximas semanas. Pero, sólo por ahora, permítanme decirles que matemáticamente es diferencial parcial ecuación que es el tiempo directivo de la densidad fila es el segundo espacial directivo de la densidad este tiempo, algunos coeficiente de difusión. Y usted aprenderá en durante la semana tres que se puede discretizar dicha ecuación y encontrar una diferencia. Y dice simplemente que la evolución en el tiempo es un valor anterior más alguna combinación de sus vecinos izquierdo y derecho. Así que sólo hay matemáticas traduciendo esta ecuación a una forma discreta. ¿De acuerdo? Pero ahora, sabemos que nuestro proceso estocástico tiene la siguiente propiedad que nos dice cómo aumenta la probabilidad de densidad en el tiempo, y ya ves que es muy similar a esta cosa. Así que básicamente, ves que este p de xt, si quiero que p sea igual a crudo, tengo una contribución aquí y una contribución aquí. Así que significa que este coeficiente 2 veces este más 1 debe ser igual a w0. Y lo mismo, puedo igualar esto con esto y esto con esto. Así que esto le da exactamente esta condición, así que siempre que elija W+ y W- uno como esta cantidad, y W0 como básicamente el complemento, así que eso es todo suma a uno. Obtiene exactamente un proceso que reproduce la ecuación de difusión. Así que es un proceso estocástico, tiene que ejecutarlo muchas veces, tiene que hacer estadística, pero si lo hace verá que su partícula que salta aleatoriamente se distribuirá exactamente como la ecuación de difusión. Lo que también es interesante en este enfoque es que ves esta condición que te dice que en realidad ya que tienes probabilidad tienes esta condición de que esta cantidad debe ser menor que la mitad de lo contrario puedes crear probabilidad negativa lo que por supuesto no es muy bueno. Es interesante saber que esta condición es también la condición de estabilidad de este esquema numérico. Así que ves que hay una bonita consistencia entre los dos enfoques. Okay, así que en realidad podemos reemplazar la solución de la ecuación de difusión por un proceso estocástico de paseo aleatorio. Okay, tal vez parece demasiado simple resolver la ecuación de difusión con el paseo aleatorio. Pero debe darse cuenta en que en su paseo aleatorio puede fácilmente añadir obstáculos o mecánica de agregación son todo tipo de, ya sabe, otra característica que podría ser difícil de insertar en la ecuación diferencial. Así que tal vez el proceso se representa realmente a nivel de la partícula. Es más fácil hacerlo como una simulación Monte Carlo en lugar de intentar resolver una complicada ecuación de difusión. Así que ahora, permítame adentrarme un poco más en el caso general, y quiero tomar de nuevo la ecuación que me dice cómo evolucionó en el tiempo la probabilidad de mi explorador, así que ya vimos esta ecuación. Y ahora, quiero transformarla de una forma muy sencilla. Así que, básicamente, voy a singularizar la situación cuando x primo es igual a x para que este término y, por supuesto, el resto es todo resultado x primo igual a x, ¿de acuerdo? Y entonces sólo uso este término que reemplazo por el hecho de que la suma de todas las W es 1, así que realmente puedo reescribirlo de esta manera. De acuerdo, y ahora agrupo estas dos sumas en una, y obtengo este tipo de ella. Entonces, esto de aquí, y así tengo esta ecuación que me dice cómo mi probabilidad de mi explorador aleatorio evolucionó en el tiempo y por supuesto de nuevo el objetivo aquí es tratar de encontrar la W para que esta p sea realmente la identidad que conoces y quieres muestrear. Y si quieres imponer que p sea igual a alguna fila que se conozca en estado estacionario, vale, es obvio que necesitas que todo este término sea igual a cero, vale, así que básicamente para que p sea igual a rho necesitas encontrar Ws que satisfagan esta ecuación. Así que esto no es fácil porque está la suma pero hay una forma de, que es más restrictiva decir que podemos este elemento, r0, que es exactamente lo que vemos aquí. Y esa condición se llama en física, equilibrio detallado porque significa que la prioridad de estar en x y saltar a x primo y saltar a x la misma que la probabilidad de estar en x y saltar a x primo, así que por eso se llama equilibrio detallado. Y el equilibrio detallado resolverá sin duda esta condición. Tal vez sea una condición suficiente, tal vez no sea una condición necesaria, pero nos quedaremos con ella por ahora. Así que la famosa regla de la metrópolis, la metrópolis es esta persona que utilizó el enfoque Monte Carlo en el Proyecto Manhattan. Le interesaba muestrear la distribución Maxwell-Boltzmann que de nuevo, estoy repitiendo, así que es la energía del sistema dividida por su temperatura. Y demostró que esta regla de transición es buena. Obedece el equilibrio de detalle y es conocida, es bien conocida como la regla de Metropolis. Así que, digamos que vas de X a X primo, con probabilidad uno, si la la nueva energía que tienes es menor que la anterior así que el sistema está contento de ir a una energía más baja. Ahora bien, si la energía de tu nueva configuración es mayor que la anterior. Puede seguir yendo pero con cierta probabilidad que disminuye a causa de este menos, a medida que la temperatura es baja, y a medida que el salto de energía es grande, ¿de acuerdo? Pero seguirá teniendo una probabilidad de ir a un estado de mayor energía, y en la forma justa de muestrear esta distribución a partir de la teoría que acabamos de ver. Entonces, en la práctica, ¿cómo lo haría en nuestro sistema de partículas? Seleccionaría una partícula al azar. Digamos que esta negra de aquí y decide moverla por una cantidad aleatoria a esta posición de aquí. Ahora como ha cambiado la posición de la partícula, ha cambiado la energía de mi gas y tengo una nueva energía E prime. Aceptaré este cambio de la partícula si corresponde a la regla de la metrópolis, que en la práctica se puede escribir así. Simplemente puede tomar un número aleatorio entre 0 y 1 y si es menor que el mínimo entre 1 y esta cantidad, acepta el cambio de lo contrario rechaza el cambio. Así que esta expresión es sólo una forma de expresar la ecuación que tenía antes aquí. De acuerdo, y entonces ¿cuál es el beneficio de hacer esto si puedo muestrear mi distribución del gas? Entonces puedo calcular algunas propiedades como la presión o cualquier propiedad física que tenga el gas, y por supuesto, eso es lo que interesaba para el Proyecto Manhattan. Así que es fácil demostrar que la regla de Metrópolis en realidad obedece al equilibrio detallado. Así que asumo simplemente que E primo es mayor que E. Así que si quiero calcular esta cantidad, así que esa es mi distribución en el equilibrio aquí, vale, veces esta tasa de transición, que su dada por la regla de Metropolis. Bien ahora puedo por supuesto combinar las dos exponenciales en una y obtengo simplemente esto, vale. Pero esto no es más que lo que llamamos la densidad para la contribución x prime veces uno si quiere. Pero uno es exactamente ahora la probabilidad de saltar de X a expandir a x porque E primo es mayor que E, así que entonces el salto sería con probabilidad uno. Así que ven que, por supuesto, se obedece al equilibrio detallado. Y para los que tengan un poco más de curiosidad, tienen otra forma de satisfacer el equilibrio detallado, que se llama Las Reglas de Glauber. Simplemente es tomar esto como una expresión. Y en el caso de un sistema equivalente se obtiene esto como una transición. Y es, por supuesto, también un equilibrio detallado base. Así que con esto me gustaría cerrar mi discusión sobre el enfoque de Monte-Carlo de cadenas de Markov. Y en el próximo módulo, discutiremos lo que se llama como método cinético o dinámico de Monte Carlo. Gracias por su atención.